{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juw5HaskLU92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1f2bf6-cb97-414a-e047-4d3a80f4f4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, TFBertModel, ViTModel\n",
        "from tensorflow.keras.layers import Dense, Dropout, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, concatenate, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycKRePH-hvc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38063c0-1544-40f9-cd5f-8007f25c5af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZuJUSjILU94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5cbfbc-c8c6-4041-d156-3d15480f2124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 2088\n"
          ]
        }
      ],
      "source": [
        "# Path to your captions file in Google Drive\n",
        "file_path = '/content/drive/My Drive/captions.txt'\n",
        "\n",
        "# Reading the captions\n",
        "captions = open(file_path, 'r').read()\n",
        "\n",
        "# Create a dictionary mapping image names to captions\n",
        "\n",
        "def load_captions(captions):\n",
        "    mapping = {}\n",
        "    for line in captions.strip().split('\\n'):\n",
        "        tokens = line.strip().split(',')\n",
        "        if len(tokens) < 2:\n",
        "            continue\n",
        "        image_id, caption = tokens[0], tokens[1]\n",
        "        image_id = image_id.split('#')[0]\n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id] = []\n",
        "        mapping[image_id].append(caption)\n",
        "    return mapping\n",
        "\n",
        "all_captions_mapping = load_captions(captions)\n",
        "print(f\"Total images: {len(all_captions_mapping)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0MWc71FLU95"
      },
      "outputs": [],
      "source": [
        "# Selecting the first 1500 images\n",
        "\n",
        "all_captions_mapping.pop('image', None)\n",
        "captions_mapping = {k: all_captions_mapping[k] for k in list(all_captions_mapping.keys())[:6000]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWCKWr85LU96"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Convert RGBA to RGB if necessary\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "\n",
        "    # Resize the image to (224, 224) regardless of its original size\n",
        "    img = img.resize((224, 224))\n",
        "\n",
        "    # Convert to numpy array and preprocess for CNN input\n",
        "    img = np.array(img)\n",
        "    img = preprocess_input(img)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv31FEolJAP6"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def clean_captions(captions_mapping):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for img_id, captions in captions_mapping.items():\n",
        "        for i, caption in enumerate(captions):\n",
        "            # Tokenize\n",
        "            caption = caption.lower()\n",
        "            caption = caption.translate(table)\n",
        "            caption = caption.strip()\n",
        "            caption = ' '.join([word for word in caption.split() if len(word)>1])\n",
        "            # Add start and end tokens\n",
        "            caption = 'startseq ' + caption + ' endseq'\n",
        "            captions[i] = caption\n",
        "\n",
        "clean_captions(captions_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYQT7MKvLU97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925dded2-8120-4afa-e27c-4d38b3235b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1840\n",
            "Maximum caption length: 23\n"
          ]
        }
      ],
      "source": [
        "# Build a list of all captions\n",
        "\n",
        "all_captions = []\n",
        "for captions in captions_mapping.values():\n",
        "    all_captions.extend(captions)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Maximum length of a caption\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "print(f\"Maximum caption length: {max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtO7ACgnLU97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74f1303-1701-45c9-f992-5d208b85e498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def load_vit_model():\n",
        "    vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "    return vit_model\n",
        "\n",
        "vit_model = load_vit_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D7TqUCxLU98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccc8749-f1cf-43b1-8bbc-528373023db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature vector shape: torch.Size([1, 197, 768])\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load ViT feature extractor and model\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# Load and preprocess the image\n",
        "def preprocess_image(img_path):\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# Path to the image\n",
        "image_path = '/content/drive/My Drive/Flickr8k/Flickr8k_Dataset/Images/973827791_467d83986e.jpg'  # Replace with an actual image path\n",
        "\n",
        "# Preprocess the image\n",
        "inputs = preprocess_image(image_path)\n",
        "\n",
        "# Pass the image through the model\n",
        "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "    outputs = vit_model(**inputs)\n",
        "\n",
        "# Extract the feature vector from the last hidden state\n",
        "feature_vector = outputs.last_hidden_state\n",
        "\n",
        "# Print the shape of the feature vector\n",
        "print(f\"Feature vector shape: {feature_vector.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0eXhGc5eAqE"
      },
      "outputs": [],
      "source": [
        "def create_sequences(tokenizer, max_length, captions_list, image_id, features):\n",
        "    X1, X2, y = [], [], []\n",
        "    for caption in captions_list:\n",
        "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq = seq[:i]\n",
        "            out_seq = seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]  # Always pad to max_length\n",
        "            out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            X1.append(features)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2D2mu46LU9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "bc302fef-dca4-4a04-a58a-077240ab175d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m471,040\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m196,864\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│                           │                        │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m131,328\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1840\u001b[0m)           │        \u001b[38;5;34m472,880\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">471,040</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│                           │                        │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1840</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">472,880</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,797,424\u001b[0m (6.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,797,424</span> (6.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,797,424\u001b[0m (6.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,797,424</span> (6.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_lstm_model(vocab_size, max_length):\n",
        "\n",
        "    # Image feature input\n",
        "    inputs1 = Input(shape=(768,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # Sequence input\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "\n",
        "    # LSTM layer with use_cudnn=False to prevent cuDNN-related padding mask errors\n",
        "    se3 = LSTM(256, use_cudnn=False)(se2)\n",
        "\n",
        "    # Decoder (combine features)\n",
        "    decoder1 = concatenate([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Build the LSTM model\n",
        "lstm_model = create_lstm_model(vocab_size, max_length)\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XogZ1sBLLU9-"
      },
      "outputs": [],
      "source": [
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkTW1cZdLU9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3837989-f07f-46cb-fa41-c2cadb6b4a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted features for 8111 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Custom dataset for loading and preprocessing images\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, images_directory, feature_extractor):\n",
        "        self.img_paths = [os.path.join(images_directory, img) for img in os.listdir(images_directory)]\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
        "        return img_inputs['pixel_values'].squeeze(0), os.path.basename(img_path)  # Return preprocessed image and filename\n",
        "\n",
        "# Extract features in batches\n",
        "def extract_vit_features_batch(vit_model, dataset, batch_size=16):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    features = {}\n",
        "\n",
        "    vit_model.eval()  # Set model to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    vit_model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_imgs, batch_names in dataloader:\n",
        "            batch_imgs = batch_imgs.to(device)\n",
        "            outputs = vit_model(batch_imgs)\n",
        "            batch_features = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Get mean of patch embeddings for each image\n",
        "            for i, img_name in enumerate(batch_names):\n",
        "                features[img_name] = batch_features[i]\n",
        "\n",
        "    return features\n",
        "\n",
        "# Initialize feature extractor and model\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# Create dataset and extract features in batches\n",
        "image_dataset = ImageDataset('/content/drive/My Drive/Flickr8k/Flickr8k_Dataset/Images', feature_extractor)\n",
        "features = extract_vit_features_batch(vit_model, image_dataset, batch_size=16)\n",
        "\n",
        "print(f\"Extracted features for {len(features)} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mthvUxXFLU9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2861061-fe4d-4c64-de60-6e4e6575cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 shape: (14555, 768), X2 shape: (14555, 23), y shape: (14555, 1840)\n"
          ]
        }
      ],
      "source": [
        "# Prepare training data\n",
        "X1, X2, y = [], [], []\n",
        "for img_id, captions_list in captions_mapping.items():\n",
        "    # Use the full image filename to retrieve the feature\n",
        "    if img_id in features:\n",
        "        feature = features[img_id]  # No need to split the img_id\n",
        "        xi1, xi2, yi = create_sequences(tokenizer, max_length, captions_list, img_id, feature)\n",
        "        X1.extend(xi1)\n",
        "        X2.extend(xi2)\n",
        "        y.extend(yi)\n",
        "    else:\n",
        "        print(f\"Warning: No features found for image {img_id}\")\n",
        "\n",
        "X1 = np.array(X1)\n",
        "X2 = np.array(X2)\n",
        "y = np.array(y)\n",
        "print(f\"X1 shape: {X1.shape}, X2 shape: {X2.shape}, y shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8R5w0pLU-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94af04d5-19e7-4272-ae8e-d513add3143e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.9379\n",
            "Epoch 1: loss improved from inf to 5.51398, saving model to model-ep001-loss5.514.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - loss: 5.9361\n",
            "Epoch 2/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.8617\n",
            "Epoch 2: loss improved from 5.51398 to 4.80381, saving model to model-ep002-loss4.804.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 4.8610\n",
            "Epoch 3/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.4454\n",
            "Epoch 3: loss improved from 4.80381 to 4.44538, saving model to model-ep003-loss4.445.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 4.4454\n",
            "Epoch 4/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.1180\n",
            "Epoch 4: loss improved from 4.44538 to 4.12848, saving model to model-ep004-loss4.128.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 4.1181\n",
            "Epoch 5/150\n",
            "\u001b[1m224/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.8137\n",
            "Epoch 5: loss improved from 4.12848 to 3.81115, saving model to model-ep005-loss3.811.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 3.8137\n",
            "Epoch 6/150\n",
            "\u001b[1m224/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4392\n",
            "Epoch 6: loss improved from 3.81115 to 3.47810, saving model to model-ep006-loss3.478.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 3.4400\n",
            "Epoch 7/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.0991\n",
            "Epoch 7: loss improved from 3.47810 to 3.13150, saving model to model-ep007-loss3.132.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 3.0999\n",
            "Epoch 8/150\n",
            "\u001b[1m227/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7388\n",
            "Epoch 8: loss improved from 3.13150 to 2.77682, saving model to model-ep008-loss2.777.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2.7391\n",
            "Epoch 9/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.3859\n",
            "Epoch 9: loss improved from 2.77682 to 2.42530, saving model to model-ep009-loss2.425.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 2.3866\n",
            "Epoch 10/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.0107\n",
            "Epoch 10: loss improved from 2.42530 to 2.10082, saving model to model-ep010-loss2.101.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2.0119\n",
            "Epoch 11/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.7703\n",
            "Epoch 11: loss improved from 2.10082 to 1.83930, saving model to model-ep011-loss1.839.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.7715\n",
            "Epoch 12/150\n",
            "\u001b[1m227/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.5286\n",
            "Epoch 12: loss improved from 1.83930 to 1.59430, saving model to model-ep012-loss1.594.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.5292\n",
            "Epoch 13/150\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3177\n",
            "Epoch 13: loss improved from 1.59430 to 1.40205, saving model to model-ep013-loss1.402.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.3181\n",
            "Epoch 14/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1796\n",
            "Epoch 14: loss improved from 1.40205 to 1.23703, saving model to model-ep014-loss1.237.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.1804\n",
            "Epoch 15/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0222\n",
            "Epoch 15: loss improved from 1.23703 to 1.08489, saving model to model-ep015-loss1.085.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.0231\n",
            "Epoch 16/150\n",
            "\u001b[1m222/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9336\n",
            "Epoch 16: loss improved from 1.08489 to 0.97897, saving model to model-ep016-loss0.979.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.9349\n",
            "Epoch 17/150\n",
            "\u001b[1m222/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8252\n",
            "Epoch 17: loss improved from 0.97897 to 0.88633, saving model to model-ep017-loss0.886.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.8271\n",
            "Epoch 18/150\n",
            "\u001b[1m224/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7526\n",
            "Epoch 18: loss improved from 0.88633 to 0.79767, saving model to model-ep018-loss0.798.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.7535\n",
            "Epoch 19/150\n",
            "\u001b[1m227/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6723\n",
            "Epoch 19: loss improved from 0.79767 to 0.72808, saving model to model-ep019-loss0.728.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.6728\n",
            "Epoch 20/150\n",
            "\u001b[1m227/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6191\n",
            "Epoch 20: loss improved from 0.72808 to 0.65156, saving model to model-ep020-loss0.652.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6194\n",
            "Epoch 21/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5723\n",
            "Epoch 21: loss improved from 0.65156 to 0.61830, saving model to model-ep021-loss0.618.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.5731\n",
            "Epoch 22/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5289\n",
            "Epoch 22: loss improved from 0.61830 to 0.56603, saving model to model-ep022-loss0.566.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.5299\n",
            "Epoch 23/150\n",
            "\u001b[1m227/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4955\n",
            "Epoch 23: loss improved from 0.56603 to 0.52877, saving model to model-ep023-loss0.529.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.4958\n",
            "Epoch 24/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4497\n",
            "Epoch 24: loss improved from 0.52877 to 0.48716, saving model to model-ep024-loss0.487.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.4507\n",
            "Epoch 25/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4331\n",
            "Epoch 25: loss improved from 0.48716 to 0.46774, saving model to model-ep025-loss0.468.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.4336\n",
            "Epoch 26/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3973\n",
            "Epoch 26: loss improved from 0.46774 to 0.44416, saving model to model-ep026-loss0.444.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.3981\n",
            "Epoch 27/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3927\n",
            "Epoch 27: loss improved from 0.44416 to 0.41623, saving model to model-ep027-loss0.416.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.3932\n",
            "Epoch 28/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3657\n",
            "Epoch 28: loss improved from 0.41623 to 0.39381, saving model to model-ep028-loss0.394.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.3664\n",
            "Epoch 29/150\n",
            "\u001b[1m222/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3397\n",
            "Epoch 29: loss improved from 0.39381 to 0.36802, saving model to model-ep029-loss0.368.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.3405\n",
            "Epoch 30/150\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3231\n",
            "Epoch 30: loss improved from 0.36802 to 0.36076, saving model to model-ep030-loss0.361.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.3233\n",
            "Epoch 31/150\n",
            "\u001b[1m222/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3204\n",
            "Epoch 31: loss improved from 0.36076 to 0.34160, saving model to model-ep031-loss0.342.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.3210\n",
            "Epoch 32/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2941\n",
            "Epoch 32: loss improved from 0.34160 to 0.33176, saving model to model-ep032-loss0.332.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.2946\n",
            "Epoch 33/150\n",
            "\u001b[1m224/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2954\n",
            "Epoch 33: loss improved from 0.33176 to 0.32624, saving model to model-ep033-loss0.326.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.2961\n",
            "Epoch 34/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2878\n",
            "Epoch 34: loss improved from 0.32624 to 0.31277, saving model to model-ep034-loss0.313.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2884\n",
            "Epoch 35/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2568\n",
            "Epoch 35: loss improved from 0.31277 to 0.28509, saving model to model-ep035-loss0.285.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2576\n",
            "Epoch 36/150\n",
            "\u001b[1m222/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2460\n",
            "Epoch 36: loss improved from 0.28509 to 0.28191, saving model to model-ep036-loss0.282.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.2471\n",
            "Epoch 37/150\n",
            "\u001b[1m222/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2424\n",
            "Epoch 37: loss improved from 0.28191 to 0.27083, saving model to model-ep037-loss0.271.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2433\n",
            "Epoch 38/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2633\n",
            "Epoch 38: loss did not improve from 0.27083\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2636\n",
            "Epoch 39/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2419\n",
            "Epoch 39: loss improved from 0.27083 to 0.26186, saving model to model-ep039-loss0.262.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.2422\n",
            "Epoch 40/150\n",
            "\u001b[1m225/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2276\n",
            "Epoch 40: loss improved from 0.26186 to 0.25374, saving model to model-ep040-loss0.254.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2280\n",
            "Epoch 41/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2310\n",
            "Epoch 41: loss improved from 0.25374 to 0.25288, saving model to model-ep041-loss0.253.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.2316\n",
            "Epoch 42/150\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2214\n",
            "Epoch 42: loss improved from 0.25288 to 0.24682, saving model to model-ep042-loss0.247.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2215\n",
            "Epoch 43/150\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2144\n",
            "Epoch 43: loss improved from 0.24682 to 0.24333, saving model to model-ep043-loss0.243.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2145\n",
            "Epoch 44/150\n",
            "\u001b[1m223/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2162\n",
            "Epoch 44: loss improved from 0.24333 to 0.23227, saving model to model-ep044-loss0.232.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.2166\n",
            "Epoch 45/150\n",
            "\u001b[1m224/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2147\n",
            "Epoch 45: loss improved from 0.23227 to 0.22971, saving model to model-ep045-loss0.230.keras\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.2150\n",
            "Epoch 46/150\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2257\n",
            "Epoch 46: loss did not improve from 0.22971\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.2257\n",
            "Epoch 47/150\n",
            "\u001b[1m226/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2249\n",
            "Epoch 47: loss did not improve from 0.22971\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2250\n",
            "Epoch 48/150\n",
            "\u001b[1m227/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2091\n",
            "Epoch 48: loss did not improve from 0.22971\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2093\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x798d7ff97910>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define the model checkpoint callback to save the best model\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}.keras'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Define the early stopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "# Fit the model with early stopping and checkpointing\n",
        "lstm_model.fit([X1, X2], y, epochs=150, batch_size=64,\n",
        "               callbacks=[checkpoint, early_stopping], verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tUlEXpmLU-A"
      },
      "source": [
        "## **5. Evaluating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJj9bWdZLU-B"
      },
      "outputs": [],
      "source": [
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        # Encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # Predict next word\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        # Map integer to word\n",
        "        word = tokenizer.index_word.get(yhat)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "image_id = list(captions_mapping.keys())[10].split('.')[0] # Convert dict_keys object to a list\n",
        "photo = features[image_id]\n",
        "photo = np.expand_dims(photo, axis=0)\n",
        "\n",
        "# Generate caption\n",
        "caption = generate_caption(lstm_model, tokenizer, photo, max_length)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open('/content/drive/My Drive/Flickr8k/Flickr8k_Dataset/Images/'+ list(captions_mapping.keys())[10]) # Convert dict_keys object to a list\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Generated caption: {caption}\")"
      ],
      "metadata": {
        "id": "XWrDQ_0dNJP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "image_id = list(captions_mapping.keys())[1044].split('.')[0] # Convert dict_keys object to a list\n",
        "photo = features[image_id]\n",
        "photo = np.expand_dims(photo, axis=0)\n",
        "\n",
        "# Generate caption\n",
        "caption = generate_caption(lstm_model, tokenizer, photo, max_length)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open('/content/drive/My Drive/Flickr8k/Flickr8k_Dataset/Images/'+ list(captions_mapping.keys())[1044]) # Convert dict_keys object to a list\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Generated caption: {caption}\")"
      ],
      "metadata": {
        "id": "_-wDHttwNQx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "image_id = list(captions_mapping.keys())[1190].split('.')[0] # Convert dict_keys object to a list\n",
        "photo = features[image_id]\n",
        "photo = np.expand_dims(photo, axis=0)\n",
        "\n",
        "# Generate caption\n",
        "caption = generate_caption(lstm_model, tokenizer, photo, max_length)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open('/content/drive/My Drive/Flickr8k/Flickr8k_Dataset/Images/'+ list(captions_mapping.keys())[1190]) # Convert dict_keys object to a list\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Generated caption: {caption}\")"
      ],
      "metadata": {
        "id": "uCXmYomvNSPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "image_id = list(captions_mapping.keys())[1128].split('.')[0] # Convert dict_keys object to a list\n",
        "photo = features[image_id]\n",
        "photo = np.expand_dims(photo, axis=0)\n",
        "\n",
        "# Generate caption\n",
        "caption = generate_caption(lstm_model, tokenizer, photo, max_length)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open('/content/drive/My Drive/Flickr8k/Flickr8k_Dataset/Images/'+ list(captions_mapping.keys())[1128]) # Convert dict_keys object to a list\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Generated caption: {caption}\")"
      ],
      "metadata": {
        "id": "pm5EkxYANTLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_model(model, captions_mapping, features, tokenizer, max_length):\n",
        "    actual, predicted = [], []\n",
        "\n",
        "    # Loop through each image ID and its corresponding captions\n",
        "    for img_id, captions_list in captions_mapping.items():\n",
        "        # Check if the image features are available\n",
        "        if img_id in features:\n",
        "            feature = features[img_id]\n",
        "            # Generate a caption for the image\n",
        "            y_pred = generate_caption(model, tokenizer, feature.reshape(1, -1), max_length)\n",
        "\n",
        "            # Prepare the reference and predicted captions\n",
        "            references = [caption.split() for caption in captions_list]  # Tokenize the actual captions\n",
        "            y_pred = y_pred.split()  # Tokenize the predicted caption\n",
        "\n",
        "            # Append to actual and predicted lists\n",
        "            actual.append(references)\n",
        "            predicted.append(y_pred)\n",
        "        else:\n",
        "            print(f\"Warning: No features found for image {img_id}\")\n",
        "\n",
        "    # Calculate BLEU scores with different weights\n",
        "    bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))  # BLEU-1\n",
        "    bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))  # BLEU-2\n",
        "    bleu3 = corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0))  # BLEU-3\n",
        "    bleu4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))  # BLEU-4\n",
        "\n",
        "    # Print all BLEU scores\n",
        "    print(f'BLEU-1: {bleu1:.4f}')\n",
        "    print(f'BLEU-2: {bleu2:.4f}')\n",
        "    print(f'BLEU-3: {bleu3:.4f}')\n",
        "    print(f'BLEU-4: {bleu4:.4f}')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_model(lstm_model, captions_mapping, features, tokenizer, max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw8yHe62HpwY",
        "outputId": "3edc0ea7-a840-4471-8582-c0abd21f7ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ]
    }
  ]
}